
<!DOCTYPE html>

<html lang="zh_CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>推理管道 &#8212; OpenVINO  文档</title>
    
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-bootstrap.5fd3999ee7762ccc51105388f4a9d115.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    <link rel="stylesheet" type="text/css" href="_static/doxyrest-pygments.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <link href="_static/css/media/favicon.ico" rel="shortcut icon">
    <link rel="stylesheet" href="_static/css/openvino_sphinx_theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/button.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/input.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/textfield.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/tabs.css" type="text/css" />
    <script src="_static/js/openvino_sphinx_theme.js"></script>
    <link rel="stylesheet" href="_static/css/viewer.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/css/custom.css" type="text/css" />

    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/chartjs-plugin-annotation/0.5.7/chartjs-plugin-annotation.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-barchart-background@1.3.0/build/Plugin.Barchart.Background.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.3.1/papaparse.min.js"></script>
    <script src="_static/js/viewer.min.js"></script>
    <script src="/assets/versions_raw.js"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/tabs.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/translations.js"></script>
    <script src="_static/target-highlight.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="配置设备" href="configure_devices.html" />
    <link rel="prev" title="模型下载程序和其他自动化工具" href="README.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="zh_CN">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
      <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="index.html">
<p class="title">OpenVINO</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="CPU.html">
  CPU 设备
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="Convert_Model_From_Paddle.html">
  转换 PadlePaddle 模型
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="Deep_Learning_Model_Optimizer_DevGuide.html">
  模型优化器用法
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="Device_Plugins.html">
  推理设备支持
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="GPU.html">
  GPU 设备
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="Getting_performance_numbers.html">
  获得性能数据
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="Intro.html">
  OpenVINO™ API 2.0 迁移指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="OTE_landing_page.html">
  OpenVINO™ 训练扩展
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="README.html">
  模型下载程序和其他自动化工具
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  推理管道
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="configure_devices.html">
  配置设备
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="deployment_guide_introduction.html">
  OpenVINO™ 部署简介
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="deployment_intro.html">
  使用 OpenVINO™ 部署应用
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="deployment_migration.html">
  安装和部署
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="dl_workbench_overview.html">
  OpenVINO™ 深度学习工作台概述
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="dldt_deployment_optimization_guide.html">
  运行时推理优化
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="dldt_optimization_guide.html">
  性能优化简介虽然推理性能应定义为许多因素（包括精度和效率）的组合，但多数情况下会将其描述为执行速度。作为模型处理实时数据的速率，从根本上说，它基于两个相互关联的指标：延迟和吞吐量。
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="dlstreamer.html">
  英特尔® Deep Learning Streamer（英特尔® DL Streamer）
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="gapi_intro.html">
  OpenCV Graph API (G-API) 简介
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="graph_construction.html">
  在 OpenVINO™ 运行时创建模型
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="home.html">
  OpenVINO™ 模型服务器
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="introduction.html">
  OpenVINO™ 安全性简介
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="model_introduction.html">
  模型处理简介
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="model_optimization_guide.html">
  模型优化指南
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="openvino_ecosystem.html">
  OpenVINO™ 生态系统概述
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="openvino_ecosystem_ovtf.html">
  OpenVINO™ 与 TensorFlow 集成
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="openvino_intro.html">
  利用 OpenVINO™ 运行时展开推理
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="ov_dynamic_shapes.html">
  动态形状
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="ov_transformations.html">
  转换 API 概述
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="ovsa_get_started.html">
  OpenVINO™ 安全附加组件
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="performance_benchmarks.html">
  性能基准测试
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="performance_hints.html">
  高级别性能提示
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="preprocessing.html">
  预处理
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="protecting_model_guide.html">
  结合使用加密模型和 OpenVINO
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="supported_model_formats.html">
  支持的模型格式
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="workbench.html">
  深度学习工作台安全性
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
</ul>
      </div>
      
      <div class="navbar-end-item">
        
<div class="dropdown sst-dropdown sst-dropdown-navbar">
  <button class="btn sst-btn dropdown-toggle" type="button" id="version-selector" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"></button>
  <div class="dropdown-menu" aria-labelledby="version-selector">
  </div>
</div>
      </div>
      
      <div class="navbar-end-item">
        

      </div>
      
    </div>
  </div>
</div>
        <div id="collapse-nav-wrapper" class="container-xl">
          <button id="collapse-nav" class="button bttn-prm button-size-m" type="button" data-toggle="collapse" data-target="#nav-tree" aria-expanded="false" aria-controls="nav-tree">
            Documentation navigation <i class="fas fa-chevron-down"></i>
          </button>
        </div>
      </nav>
      <div class="transition-banner container-fluid alert alert-info alert-dismissible fade show" role="alert">
        <p>OpenVINO 2022.1 introduces a new version of OpenVINO API (API 2.0). For more information on the changes and transition steps, see the <a href="https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html">transition guide</a></p>
        <button type="button" class="close" data-dismiss="alert" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
    </div>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar" id="nav-tree"><form class="searchForm bd-search d-flex align-items-center" action="search.html" method="get">
    <i class="icon fas fa-search"></i>
    <input type="search" class="form-control" name="query" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-create-core-a-1">
   <a name="create-core">
   </a>
   1. 创建核心对象
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-load-extensions-a-1-1">
     <a name="load-extensions">
     </a>
     1.1 （可选）加载扩展
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-read-model-a-2">
   <a name="read-model">
   </a>
   2.从驱动器中读取模型
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-perform-preprocessing-a-2-1">
     <a name="perform-preprocessing">
     </a>
     2.1 （可选）执行模型预处理
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-load-model-to-device-a-3">
   <a name="load-model-to-device">
   </a>
   3.将模型加载到设备
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-create-inference-request-a-4">
   <a name="create-inference-request">
   </a>
   4.创建推理请求
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-fill-tensor-a-5">
   <a name="fill-tensor">
   </a>
   5.用数据填充输入张量
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-start-inference-a-6">
   <a name="start-inference">
   </a>
   6.开始推理
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-name-process-results-a-7">
   <a name="process-results">
   </a>
   7.处理推理结果
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                <div class="tocsection download-docs">
  <div class="dropdown sst-dropdown">
    <button class="button bttn-prm button-size-m" data-display="static" type="button" id="download-options"
      data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
      Download Docs
    </button>
    <div class="dropdown-menu" aria-labelledby="download-options">
      <a class="dropdown-item" href="#" onclick="window.print()">.pdf</a>
      <a id="download-zip-btn" class="dropdown-item" href="#">.zip</a>
    </div>
  </div>
</div>
              </div>
              
            
          </div>
          

          
          
              
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
            
                <div>
                  
  <section id="id1">
<h1>推理管道<a class="headerlink" href="#id1" title="永久链接至标题">¶</a></h1>
<p>如需使用 OpenVINO™ 运行时推理模型，通常您需要在应用管道中执行以下步骤：</p>
<ol class="arabic simple">
<li><p><a href="#create-core">创建核心对象。</a></p>
<ul class="simple">
<li><p>1.1. <a href="#load-extensions">（可选）加载扩展。</a></p></li>
</ul>
</li>
<li><p><a href="#read-model">从驱动器中读取模型。</a></p>
<ul class="simple">
<li><p>2.1. <a href="#perform-preprocessing">（可选）执行模型预处理。</a></p></li>
</ul>
</li>
<li><p><a href="#load-model-to-device">将模型加载到设备。</a></p></li>
<li><p><a href="#create-inference-request">创建推理请求。</a></p></li>
<li><p><a href="#fill-tensor">用数据填充输入张量。</a></p></li>
<li><p><a href="#start-inference">开始推理。</a></p></li>
<li><p><a href="#process-results">处理推理结果。</a></p></li>
</ol>
<p>以下代码将基于这些步骤演示如何更改应用代码以迁移到 API 2.0。</p>
<section id="a-name-create-core-a-1">
<h2><a name="create-core"></a>1. 创建核心对象<a class="headerlink" href="#a-name-create-core-a-1" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:create_core
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:create_core
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:create_core
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:create_core
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<section id="a-name-load-extensions-a-1-1">
<h3><a name="load-extensions"></a>1.1 （可选）加载扩展<a class="headerlink" href="#a-name-load-extensions-a-1-1" title="永久链接至标题">¶</a></h3>
<p>如需通过自定义操作加载模型，您需要为这些操作添加扩展。强烈建议您使用 <span class="xref myst">OpenVINO™ 扩展性 API</span> 编写扩展。但是，您也可以将旧扩展加载到新的 OpenVINO™ 运行时：</p>
<p><strong>推理引擎 API</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:load_old_extension
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:load_old_extension
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:load_old_extension
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:load_old_extension
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
</section>
</section>
<section id="a-name-read-model-a-2">
<h2><a name="read-model"></a>2.从驱动器中读取模型<a class="headerlink" href="#a-name-read-model-a-2" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:read_model
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:read_model
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:read_model
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:read_model
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>读取模型采用与<a class="reference internal" href="graph_construction.html"><span class="doc std std-doc">模型创建迁移指南</span></a>中的示例相同的结构。</p>
<p>您可以在单次 <code class="docutils literal notranslate"><span class="pre">ov::Core::compile_model(filename,</span> <span class="pre">devicename)</span></code> 调用中组合模型读取和编译。</p>
<section id="a-name-perform-preprocessing-a-2-1">
<h3><a name="perform-preprocessing"></a>2.1 （可选）执行模型预处理<a class="headerlink" href="#a-name-perform-preprocessing-a-2-1" title="永久链接至标题">¶</a></h3>
<p>当应用输入数据与模型输入格式不完全匹配时，可能需要进行预处理。请参阅 <a class="reference internal" href="preprocessing.html"><span class="doc std std-doc">API 2.0 中的预处理</span></a>了解更多详情。</p>
</section>
</section>
<section id="a-name-load-model-to-device-a-3">
<h2><a name="load-model-to-device"></a>3.将模型加载到设备<a class="headerlink" href="#a-name-load-model-to-device-a-3" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:compile_model
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:compile_model
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:compile_model
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:compile_model
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>如果需要用 OpenVINO™ 运行时的其他参数配置设备，请参阅<a class="reference internal" href="configure_devices.html"><span class="doc std std-doc">配置设备</span></a>。</p>
</section>
<section id="a-name-create-inference-request-a-4">
<h2><a name="create-inference-request"></a>4.创建推理请求<a class="headerlink" href="#a-name-create-inference-request-a-4" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:create_infer_request
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:create_infer_request
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:create_infer_request
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:create_infer_request
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
</section>
<section id="a-name-fill-tensor-a-5">
<h2><a name="fill-tensor"></a>5.用数据填充输入张量<a class="headerlink" href="#a-name-fill-tensor-a-5" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>推理引擎 API 用 <code class="docutils literal notranslate"><span class="pre">I32</span></code> 精度（与原始模型<strong>不</strong>一致）的数据填充输入：</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{IR v10}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{IR v11}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{ONNX}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{使用代码创建的模型}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_input_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>API 2.0 用 <code class="docutils literal notranslate"><span class="pre">I64</span></code> 精度（与原始模型一致）的数据填充输入：</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{IR v10}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_input_tensor_v10
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_input_tensor_v10
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{IR v11}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_input_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_input_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{ONNX}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_input_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_input_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{使用代码创建的模型}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_input_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_input_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
</section>
<section id="a-name-start-inference-a-6">
<h2><a name="start-inference"></a>6.开始推理<a class="headerlink" href="#a-name-start-inference-a-6" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{Sync}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:inference
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:inference
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Async}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:start_async_and_wait
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:start_async_and_wait
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{Sync}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:inference
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:inference
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Async}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:start_async_and_wait
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:start_async_and_wait
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
</section>
<section id="a-name-process-results-a-7">
<h2><a name="process-results"></a>7.处理推理结果<a class="headerlink" href="#a-name-process-results-a-7" title="永久链接至标题">¶</a></h2>
<p><strong>推理引擎 API</strong></p>
<p>推理引擎 API 处理输出的原因是，输出精度为 <code class="docutils literal notranslate"><span class="pre">I32</span></code>（与原始模型<strong>不</strong>一致）：</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{IR v10}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{IR v11}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{ONNX}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{使用代码创建的模型}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ie_common.cpp ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ie_common.py ie:get_output_tensor
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p><strong>API 2.0</strong></p>
<p>API 2.0 处理输出的原因如下：</p>
<ul class="simple">
<li><p>对于 OpenVINO™ IR v10 模型，输出精度为 <code class="docutils literal notranslate"><span class="pre">I32</span></code>（与原始模型<strong>不</strong>一致），目的是匹配 <a href="openvino_2_0_transition_guide#differences-api20-ie">旧行为</a>。</p></li>
<li><p>对于 OpenVINO™ IR v11、ONNX、ov::Model 和 PaddlePaddle 模型，由于输出精度为 <code class="docutils literal notranslate"><span class="pre">I64</span></code>（与原始模型一致），目的是匹配 <a href="openvino_2_0_transition_guide#differences-api20-ie">新行为</a>。</p></li>
</ul>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{IR v10}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_output_tensor_v10
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_output_tensor_v10
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{IR v11}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_output_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_output_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{ONNX}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_output_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_output_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;sphinxtab{使用代码创建的模型}</p>
<p>&#64;sphinxtabset</p>
<p>&#64;sphinxtab{C++}
&#64;snippet docs/snippets/ov_common.cpp ov_api_2_0:get_output_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;sphinxtab{Python}
&#64;snippet docs/snippets/ov_common.py ov_api_2_0:get_output_tensor_aligned
&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
<p>&#64;endsphinxtab</p>
<p>&#64;endsphinxtabset</p>
</section>
</section>


                </div>
            
            
                <div class='prev-next-bottom'>
                  
    <a class='button bttn-sec button-size-l' id="prev-link" href="README.html" title="previous page">Prev</a>
    <a class='button bttn-sec button-size-l' id="next-link" href="configure_devices.html" title="next page">Next</a>

                </div>
            
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Various.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>